{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Feature Selection Tutorial\n",
    "Welcome to the Ensemble Feature Selection Tutorial! In this tutorial, we will guide you through the in details possibility of using the ensemblefeatureselection library to perform MultiObjective Ensemble Feature Selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and DataProcessor module \n",
    "\n",
    "To illustrate our method we will use the wine dataset from sklearn.\n",
    "\n",
    "Using the DataProcessor module we can easily go from pd.DataFrame or directly from a csv file to a processed dataset. It allows to encode categorical variable, to drop columns, to normalize non categorical columns, modify the name of the target column to 'target' as it is necessary for the pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ensemblefs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_wine\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mensemblefs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataProcessor\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load Wine dataset\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ensemblefs'"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from ensemblefs.core import DataProcessor\n",
    "import pandas as pd\n",
    "\n",
    "# Load Wine dataset\n",
    "wine_data = load_wine()\n",
    "wine_df = pd.DataFrame(data= wine_data.data, columns= wine_data.feature_names)\n",
    "wine_df['target'] = wine_data.target\n",
    "\n",
    "# process data using DataProcessor module\n",
    "dp = DataProcessor(\n",
    "    categorical_columns=['target'],\n",
    "    columns_to_drop=['nonflavanoid_phenols'],\n",
    "    drop_missing_values=True,\n",
    "    normalize=True,\n",
    "    target_column='target'\n",
    ")\n",
    "\n",
    "processed_data = dp.preprocess_data(wine_df)\n",
    "processed_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selector and Merging Strategy \n",
    "\n",
    "Each feature selector and merging strategy is encapsulated within its own class, which contains specific functionality. In the simple example tutorial, we accessed these classes using string identifiers. However, for more advanced usage, users can directly instantiate feature selectors, allowing for the specification of more detailed parameters.\n",
    "\n",
    "For example, in the code snippet below, the `RandomForestSelector` utilizes the sklearn random forest module. By creating an instance called `rf_selector`, users can pass advanced parameters using keyword arguments. This instance can then be passed directly to the pipeline, offering similar flexibility with merging strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ensemblefs.feature_selectors import RandomForestSelector, FStatisticSelector\n",
    "from ensemblefs.merging_strategies import UnionOfIntersectionsMerger\n",
    "\n",
    "# Feature Selector\n",
    "rf_selector = RandomForestSelector(task='classification', num_features_to_select=5, n_estimators=1000, max_depth=5, min_samples_leaf=2)\n",
    "f_selector = FStatisticSelector(task='classification', num_features_to_select=5)\n",
    "\n",
    "# Merging Strategy\n",
    "union_merger = UnionOfIntersectionsMerger()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then initialize the pipeline with the defined parameters and execute it.\n",
    "\n",
    "In the example below, we have introduced a new parameter, `min_group_size`, which ensures that each generated group within the pipeline contains a minimum of three feature selectors. This is an advanced feature; typically, the minimum size is set to two, representing the smallest possible ensemble. However, there may be cases where setting a higher minimum size is desirable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pipeline Progress: 100%|██████████| 5/5 [00:15<00:00,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The selected features are {'alcohol', 'color_intensity', 'od280/od315_of_diluted_wines', 'flavanoids', 'proline'}, from repeat 3 and group ('RandomForest', 'FStatistic')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from ensemblefs import FeatureSelectionPipeline\n",
    "\n",
    "num_repeats = 5\n",
    "task = \"classification\"\n",
    "num_features_to_select = 6\n",
    "\n",
    "pipeline = FeatureSelectionPipeline(\n",
    "    data=wine_df,\n",
    "    fs_methods=[rf_selector, f_selector, \"mutual_info_selector\", \"svm_selector\"],\n",
    "    merging_strategy=union_merger,\n",
    "    num_repeats=num_repeats,\n",
    "    task=task,\n",
    "    num_features_to_select=num_features_to_select,\n",
    "    min_group_size=3\n",
    ")\n",
    "\n",
    "selected_features, best_repeat, best_group = pipeline.run()\n",
    "\n",
    "print(f\"The selected features are {selected_features}, from repeat {best_repeat} and group {best_group}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "The **performance metrics** used to assess feature subsets during the pipeline are another area for customization. A pipeline object requires a list of three implemented metrics, which can be passed as the `metrics` parameter during initialization.\n",
    "\n",
    "Several metrics are already implemented, and you can also define your own.\n",
    "\n",
    "#### **Implemented Classification Metrics**\n",
    "1. **Log-loss**\n",
    "2. **Accuracy**\n",
    "3. **F1 Score**\n",
    "4. **Recall**\n",
    "5. **Precision**\n",
    "\n",
    "#### **Implemented Regression Metrics**\n",
    "1. **Mean Absolute Error (MAE)**\n",
    "2. **Mean Squared Error (MSE)**\n",
    "3. **R² Score**\n",
    "\n",
    "#### **How to Specify Metrics**\n",
    "You can provide metrics as strings or as instantiated objects. For example:\n",
    "\n",
    "```python\n",
    "from ensemblefs.metrics import RecallScore\n",
    "\n",
    "recall = RecallScore()\n",
    "\n",
    "pipeline = FeatureSelectionPipeline(\n",
    "    data=wine_df,\n",
    "    fs_methods=[rf_selector, f_selector, \"mutual_info_selector\", \"svm_selector\"],\n",
    "    merging_strategy=union_merger,\n",
    "    num_repeats=num_repeats,\n",
    "    metrics=[recall, \"precision\", \"accuracy\"],\n",
    "    task=task,\n",
    "    num_features_to_select=num_features_to_select,\n",
    "    min_group_size=3,\n",
    ")\n",
    "```\n",
    "\n",
    "This approach allows flexibility, enabling you to combine predefined metrics with your custom implementations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Custom Feature Selector or Merging Strategy\n",
    "\n",
    "1. **Inherit from the Appropriate Base Class**: Your new class should inherit from either the `FeatureSelector` base class (for feature selectors) or the `MergingStrategy` base class (for merging strategies).\n",
    "\n",
    "2. **Implement Required Methods**: \n",
    "   - For **feature selectors**, implement a method called `.compute_score()`, which will be responsible for evaluating and selecting features based on their importance.\n",
    "   - For **merging strategies**, implement a method called `.merge()`, which will handle the merging of selected features from different selectors.\n",
    "\n",
    "3. **Instance-Based Usage**: Note that the pipeline currently operates with instances of your classes, as string identifiers for class names are not implemented. Therefore, you will need to directly instantiate your custom feature selector or merging strategy when using it in the pipeline.\n",
    "\n",
    "4. **Modify Class Mapping if Necessary**: If you want to integrate your new class with a string identifier for easier usage in the future, you will need to modify the class mapping in the `utils.py` file accordingly.\n",
    "\n",
    "This structure allows you to customize the feature selection process to meet your specific needs. For detailed guidance on how to implement these classes, please refer to the documentation or relevant tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ensemblefs.feature_selectors import FeatureSelector\n",
    "import numpy as np\n",
    "\n",
    "class CustomSelector(FeatureSelector):\n",
    "    name = \"CustomSelector\"\n",
    "    def __init__(self, task, num_features_to_select=None, **kwargs):\n",
    "        super().__init__(task, num_features_to_select, **kwargs)\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def compute_scores(self, X, y):\n",
    "        # Example: Return random feature importance scores for each feature\n",
    "        # Replace this with the actual logic for calculating feature importance\n",
    "        return np.random.rand(X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ensemblefs.merging_strategies import MergingStrategy\n",
    "\n",
    "class CustomMerger(MergingStrategy):\n",
    "    \"\"\"\n",
    "    A custom merging strategy example that inherits from the MergingStrategy base class.\n",
    "    This class demonstrates how to define your own merging logic.\n",
    "    \"\"\"\n",
    "    name = \"CustomMerger\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        # Specify if the merging strategy is \"set-based\" or \"rank-based\"\n",
    "        super().__init__(\"set-based\")\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def merge(self, subsets, k_features=None):\n",
    "        # Example: Define custom merging logic here\n",
    "        # This could be a unique way of combining feature sets or ranks\n",
    "        # Return a final list of selected features based on the merging logic\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And your done! Now you can create an instance and you your custom feature selector or merging stragegy with other methods inside the pipeline! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "efs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
